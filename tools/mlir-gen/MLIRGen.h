//===- MLIRGen.h MLIR Generator -------------------------------------------===//
//
// Class that handles MLIR generation for the MLIR options.
//
//===----------------------------------------------------------------------===//

#pragma once

#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/Utils/StructuredOpsUtils.h"
#include "mlir/IR/Builders.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/Location.h"
#include "mlir/Support/LogicalResult.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringRef.h"

#include "TPP/Transforms/Utils/BuilderUtils.h"

namespace mlir {
class ModuleOp;
class MemRefType;
class Operation;
class Value;
namespace func {
class FuncOp;
} // namespace func

/// MLIR Generator: produces MLIR linalg-on-tensor dialect for an MLIR model
/// with the appropriate number of hidden layers and other properties selected.
class MLIRGenerator {
  /// MLIR Context
  MLIRContext context;

  /// MLIR OpBuilder
  OpBuilder builder;

  /// Unknown location, since all this code is auto-generated
  Location loc;

  /// Main module
  ModuleOp module;

  /// Batch size
  unsigned batch;

  /// Layer sizes
  SmallVector<int64_t> layers;

  /// Tile sizes
  SmallVector<int64_t> tiles;

  /// Data type (element type of all tensors)
  Type dataType;

  /// Random seed
  int seed;

  /// Generated model's flops
  int64_t flops;

  /// Tensor init type
  TensorInitType initType;

  // ============================ Code Generation Options

  /// Lower bias add on every layer
  bool enableBias;

  /// Lower ReLU on every layer
  bool enableRelu;

  /// Lower softmax at the last layer
  bool enableSoftmax;

  /// List of supported kernel types that can be generated
  ///  * Const: Generates weights and biases as constant (RO).
  ///  * Args: Generates weights and biaseds as arguments (RW).
  enum class KernelType { Const, Args };

  /// Type of kernel to be generated
  KernelType kernelType;

  /// VNNI packing factor (0, 2, 4)
  int vnniFactor;

  // ============================ Helpers

  /// Return current random seed, update next
  int getRand();

  /// Type of packing (NxC, KxC, NxK)
  enum PackingType { PACK_INPUT, PACK_WEIGHT, PACK_OUTPUT };

  /// Return shaped type (packed if requested)
  TensorType getShape(ArrayRef<int64_t>, PackingType);

  /// Return a zero-init tensor for matmul outputs
  Value getZeroInitTensor(TensorType);

  /// Affine expressions for maps
  SmallVector<AffineExpr, 6> affineExprs;

  enum MapType {
    MAP_PARALLEL,
    MAP_REDUCTION,
    MAP_BROADCAST,
    MAP_MATMUL_INPUT,
    MAP_MATMUL_WEIGHT,
    MAP_MATMUL_OUTPUT,
    MAP_MATMUL // Alias for iterator type
  };

  /// Return affine map (packed if requested)
  /// If order is not empty, re-order the dims in that order
  /// If dims is passed, force number of dims, otherwise, take from tensor
  /// If reduction is true, emit zeroExpr for the tail reduction
  AffineMap getMap(Value, MapType);

  /// Return the iterator types for a particular map type
  /// Add iterators if the types are packed
  SmallVector<utils::IteratorType> getIterators(MapType);

  // ============================ Core Logic
  // To avoid allocating new tensors, we bind the output of the matmul to the
  // input of the bias add, make it in-place and bind that to the input of
  // the ReLU, also making it in-place, and returning the first alloc.

  /// Creates a matmul in the current function
  /// Args: A, B, C
  /// Returns the chain value to be used in the next op
  Value lowerMatmul(Value, Value, Value);

  /// Creates a bias add in the current function
  /// Args: Input, Output (same for in-place)
  /// Returns the chain value to be used in the next op
  Value lowerBiasAdd(Value, Value, Value);

  /// Creates a relu in the current function
  /// Args: Input, Output (same for in-place)
  /// Returns the chain value to be used in the next op
  Value lowerRelu(Value, Value);

  /// Creates a softmax in the current function
  /// Args: Input, Output (same for in-place)
  /// Returns the chain value to be used in the next op
  Value lowerSoftmax(Value, Value);

  // ============================ Main API

  /// Creates metadata string containing run command, flops info etc.
  std::string createMetadata();

  /// Types are created first, values are created from the types if inside the
  /// function, or populated later from function arguments if external.
  struct Arg {
    Value value;
    TensorType type;
  };

  /// There could be multiple layers, each with its own weights and biases
  /// Input of one layer is the output of the previous
  /// Input of the model is the input of the first layer
  /// Output of the model is the output of the last layer
  struct LayerArgs {
    unsigned index;
    Arg input;
    Arg weight;
    Arg bias;
    Arg output;
  };

  /// Some arguments are optional, so we use this struct to simplify the
  /// argument handling in createLayer.
  typedef SmallVector<LayerArgs, 3> KernelArgs;

  /// Creates the kernel types from layer definitions and options
  void getKernelTypes(KernelArgs &);

  /// Creates a layer function, to be called by the kernel
  Value createLayer(LayerArgs &);

  /// Creates a kernel (N * {GEMM + AddBias + ReLU} + Softmax)
  /// AddBias, ReLU and Softmax are optional
  void createKernel();

public:
  /// Creates a specific module. Different configurations need different modules
  /// so should create new objects to not have to share / cleanup existing MLIR
  /// modules.
  MLIRGenerator(StringRef, unsigned, StringRef, StringRef, unsigned, int, bool,
                bool, bool, int);

  ~MLIRGenerator() { module->destroy(); }

  /// Generates the whole IR and write to file
  /// Return 0 on success, 1 on failure
  int generate(StringRef filename);
};

} // namespace mlir
